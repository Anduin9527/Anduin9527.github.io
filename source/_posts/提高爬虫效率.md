---
title: 提高爬虫效率
tags:
  - 爬虫
  - Python
categories: 爬虫学习
abbrlink: ee385c19
date: 2022-07-10 20:54:00
---



## 前言

爬都可以爬

<img src="https://imgbed-1304793179.cos.ap-nanjing.myqcloud.com/typora/xigua.png" alt="99586772_p0" style="zoom: 50%;" />

<!--more-->

## 线程？进程？

简单地来说，**进程**是**系统进行资源调度和分配的的基本单位**，是**资源单位**。比如任务管理器里面管理的就是一堆进程。而**线程**则是进程的子任务，**是CPU调度和分派的基本单位**，是**执行单位**。显然一个进程可以有多个线程。

### 线程与进程的小栗子

`Python`中最简单的一个多线程例子：

```python
from threading import Thread

def func():
  for i in range(500):
    print("Hello new World")

def main():
  t = Thread(target=func)  # 创建新线程并安排任务
  t.start()  # 标记线程为可以启动状态，但具体启动时间由系统决定
  for i in range(500):
    print("Main thread")

if __name__ == '__main__':
  main()
```

或者也可以像`Java`中常做的那样，重写一下`run`方法

`Python`中最简单的一个多线程例子：

```python
from multiprocessing import Process

def func():
  for i in range(500):
    print("Hello new World")

def main():
  p = Process(target=func)
  p.start()
  for i in range(500):
    print("Main process")

if __name__ == '__main__':
  main()

```

+ 不难发现，API长得几乎一样
+ 如果想要对函数传参的话，需要使用`p = Process(target=func,args=(tuple))`的形式，线程同理

### 线程池和进程池

一次性开辟若干个线程（进程），用户只需要给线程池（进程池）提交任务即可。对于具体的线程调度不需要关心

`Python`的线程池

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

def fn(name):
  for i in range(1000):
    print(name, i)


if __name__ == '__main__':
  # 创建线程池
  with ThreadPoolExecutor(max_workers=10) as t:
    for i in range(100):
      t.submit(fn, "Thread-" + str(i))
  # 等待所有线程结束，才继续执行守护
  print("ThreadPoolExecutor done")
```

+ 进程池的话，就换成`ProcessPoolExecutor`即可

### 使用线程池进行爬取

对[北京新发地的菜价](http://xinfadi.com.cn/priceDetail.html)进行一个爬取。首先要分析一下网页，发现他的数据来源是通过更改`id="current"`标签的`value`值实现的，然后使用网络工具抓包，发现其数据是请求另一个URL后进行渲染的。所以我们请求数据页后将数据记录于csv文件中。

```python
import requests
import csv
import time

f = open('xinfadi.csv', mode='w', encoding='utf-8')
csvw = csv.writer(f)

def get_one_page(value):
  proxy = {'http': 'http://127.0.0.1:7890',
           'https': 'http://127.0.0.1:7890'}
  url = "http://xinfadi.com.cn/getPriceData.html"
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',
    'referer': "http://xinfadi.com.cn/priceDetail.html"}
  data = {
    'current': value}
  resp = requests.post(url, proxies=proxy, headers=headers, data=data)
  resp.encoding = 'utf-8'
  res = resp.json()['list']
  for i in res:
    # 写入csv
    csvw.writerow(i.values())
  print(f'the {value} page done')


if __name__ == '__main__':
  # 计算时间
  start = time.time()
  for i in range(1, 100):
    get_one_page(i)
  end = time.time()
  print(f'总共用时{end - start}')
```

试着爬取100页的数据，发现耗时为68.33s，而数据库有16000页。这显然太慢了。所以使用线程池的方式进行一个改写。

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
if __name__ == '__main__':
  # 计算时间
  start = time.time()
  # 使用线程池
  with ThreadPoolExecutor(max_workers=50) as t:
    for i in range(1, 100):
      t.submit(get_one_page, i)
  end = time.time()
  print(f'总共用时{end - start}')
```

使用了50个线程的表现为12s，显然有质的提升

## 协程？

众所周知，当程序处于I/O操作时，线程往往会处于堵塞状态。比如用 requests 发请求，或者读写数据库。如果我们能在堵塞状态时也能执行别的事情，那么效率就会提高。协程就是可以当线程堵塞时，选择性地切换到别的任务，提高CPU的利用率。所以即使在**单线程**的条件下，我们也能看到多个任务"同时"进行的现象。

### 多任务异步协程

下面举个经典例子来说明协程的含金量

```python
import asyncio
import time

async def func1():
  print('func1')
  await asyncio.sleep(1)
  print('func1')


async def func2():
  print('func2')
  await asyncio.sleep(2)
  print('func2')


async def func3():
  print('func3')
  await asyncio.sleep(3)
  print('func3')


if __name__ == '__main__':
  start = time.time()
  loop = asyncio.get_event_loop()
  tasks = [func1(), func2(), func3()]
  loop.run_until_complete(asyncio.wait(tasks))
  loop.close()
  end = time.time()
  print(f'总共用时{end - start}')
```

1. 将任务声明为协程对象，即使用`async def`的方式定义函数。任何调用该函数的行为返回的都是一个`coroutine object`，而非运行其中的代码。如果想要运行其中的函数，则需要满足进入`async`模式且`coroutine`变为`task`
2. 正常的Python代码都是`sync`也就是同步模式，想要切换到`async`模式。我们通常使用`asyncio.run()`（Python >= 3.7），其参数为`coroutine`，它会建立`event loop`并把参数`coroutine`变为其中的第一个`task`开始运行 
3. 能被 `await`的对象有3种：`coroutine`、`task`和`future`。
4. 当你`await coroutine`时，$Don't \ do\  that$。直接这样做就与同步别无二致。
5. 所以我们需要直接`await task`，`event loop`就直接给出控制权，并在结束时记录返回值。这也就需要我们提前使用`asyncio.create_task(coroutine obj)`注册`task`
6. 那么如果有很多`task`需要注册呢？则可以用`asyncio.gather()`。其会返回一个`future`，参数为若干个可`await`的对象。`task`会被注册到`event loop`中，如果是`coroutine`则首先会被包装成`task`再注册到`event loop`中。然后返回的`future`其目的是告知`event loop`需要完成其中所有的`task`后才能继续执行。最后返回其中所有`task`的`return`值按照顺序返回到一个`list`中。

### aiohttp

因为异步中不能使用`requests`，所以需要使用`aiohttp`，下面是稍加改动后的官网的例子

```python
import aiohttp
import asyncio

async def fetch(session, url):
  async with session.get(url) as response:
    return await response.text(), response.status

async def main():
  async with aiohttp.ClientSession() as session:
    text, status_code = await fetch(session, "http://httpbin.org/get")
    print(f"text:{text[:100]}")
    print(f"code:{status_code}")

if __name__ == '__main__':
  asyncio.run(main())
```

大体上与`request.session`的操作方式相同。首先用`async with aiohttp.ClientSession() as session:`注册一个支持异步的上下文客户端会话管理器`session`，用`session`去异步地发送请求`  async with session.get(url) as response:`此时可以添加`params,headers,data,cookies`。最后使用`await resp.json() or .text() or .read() `来得到数据

下面给出一个常用框架：

```python
import aiohttp
import asyncio


def get_tasks(session):
  """
  获取任务列表
  :param 客户端会话
  :return: 任务列表
  """
  tasks = []
  for i in range(30):
    tasks.append(fetch(session))
  return tasks


async def fetch(session):
  """
  异步获取网页的具体过程
  :param 客户端会话
  :return: 网页的数据
  """
  async with session.get('http://httpbin.org/get') as resp:
    # 断言，如果状态码不是 200，则抛出异常
    assert resp.status == 200
    return await resp.json()


async def main():
  results = []
  async with aiohttp.ClientSession() as session:
    tasks = get_tasks(session)
    responses = await asyncio.gather(*tasks)
    print(responses)


asyncio.run(main())
```



## 参考

https://www.kingname.info/2020/03/23/insert-sprit/

https://www.bilibili.com/video/BV1oa411b7c9